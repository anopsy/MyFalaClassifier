{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1J+f7s9L1y1mfKBna2EVU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anopsy/MyFala/blob/main/wave_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "I mount my google drive, where I store data\n",
        "\n"
      ],
      "metadata": {
        "id": "SOMYXOMut6dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "1RuFgJYqq7yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cd3gTcQqNVS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_dir = 'waves'\n",
        "\n",
        "print(\"Contents of base directory:\")\n",
        "print(os.listdir(base_dir))\n",
        "\n",
        "print(\"\\nContents of train directory:\")\n",
        "print(os.listdir(f'{base_dir}/train'))\n",
        "\n",
        "print(\"\\nContents of validation directory:\")\n",
        "print(os.listdir(f'{base_dir}/validation'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating directories for later use"
      ],
      "metadata": {
        "id": "y0-54PhJvkmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with training good_waves/bad_waves pictures\n",
        "train_gw_dir = os.path.join(train_dir, 'good_waves')\n",
        "train_bw_dir = os.path.join(train_dir, 'bad_waves')\n",
        "\n",
        "# Directory with validation good_waves/bad_waves pictures\n",
        "validation_gw_dir = os.path.join(validation_dir, 'good_waves')\n",
        "validation_bw_dir = os.path.join(validation_dir, 'bad_waves')"
      ],
      "metadata": {
        "id": "5eA8Y7uPq7J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking on file names and number of images"
      ],
      "metadata": {
        "id": "1YF67IEuvrkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gw_fnames = os.listdir( train_gw_dir )\n",
        "train_bw_fnames = os.listdir( train_bw_dir )\n",
        "\n",
        "print(train_gw_fnames[:10])\n",
        "print(train_bw_fnames[:10])"
      ],
      "metadata": {
        "id": "wxSt6uZkrnFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training good waves images :', len(os.listdir(      train_gw_dir ) ))\n",
        "print('total training bad waves images :', len(os.listdir(      train_bw_dir ) ))\n",
        "\n",
        "print('total validation good waves images :', len(os.listdir( validation_gw_dir ) ))\n",
        "print('total validation bad waves images :', len(os.listdir( validation_bw_dir ) ))"
      ],
      "metadata": {
        "id": "ofs4b3pYsMZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting examples of images in each directory"
      ],
      "metadata": {
        "id": "dT7DD1M9v2Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "pic_index = 0 # Index for iterating over images"
      ],
      "metadata": {
        "id": "T44C07Mfsiw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols*4, nrows*4)\n",
        "\n",
        "pic_index+=8\n",
        "\n",
        "next_gw_pix = [os.path.join(train_gw_dir, fname) \n",
        "                for fname in train_gw_fnames[ pic_index-8:pic_index] \n",
        "               ]\n",
        "\n",
        "next_bw_pix = [os.path.join(train_bw_dir, fname) \n",
        "                for fname in train_bw_fnames[ pic_index-8:pic_index]\n",
        "               ]\n",
        "\n",
        "for i, img_path in enumerate(next_gw_pix+next_bw_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "udzM_xRps0Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple baseline model"
      ],
      "metadata": {
        "id": "VSGPWzR2v_pF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(), \n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'), \n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "])"
      ],
      "metadata": {
        "id": "Dh1Ut8o_tQ2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying model characteristics"
      ],
      "metadata": {
        "id": "Id7LA0MjwC5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "kVpJHYfWtvKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the optimisation method and metrics"
      ],
      "metadata": {
        "id": "DHavpLLYwYg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "Az0ZkxlittMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing"
      ],
      "metadata": {
        "id": "kAUHnfaiwhhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "# --------------------\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))     \n",
        "# --------------------\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode  = 'binary',\n",
        "                                                         target_size = (150, 150))"
      ],
      "metadata": {
        "id": "rm99VLGZtqiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model for 15 epochs"
      ],
      "metadata": {
        "id": "iQ-ALMS6wpEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "            train_generator,\n",
        "            epochs=15,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=2\n",
        "            )"
      ],
      "metadata": {
        "id": "OsjIz3mjuzQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gWztoENAw7oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload an image and let the model ppredict if it's a good wave or a bad wave"
      ],
      "metadata": {
        "id": "VAK6PQZJw2Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  x=img_to_array(img)\n",
        "  x /= 255\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "  \n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a good wave\")\n",
        "  else:\n",
        "    print(fn + \" is a bad_wave\")"
      ],
      "metadata": {
        "id": "UMeOATe1vFvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing training stats"
      ],
      "metadata": {
        "id": "8NHXypDew9El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ],
      "metadata": {
        "id": "5RUnWBbGvc3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up"
      ],
      "metadata": {
        "id": "i_5kCLTTxC2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, signal\n",
        "\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "metadata": {
        "id": "NsmcEZ4rvdn_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}